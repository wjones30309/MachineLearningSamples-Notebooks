{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook describes creation of a forecasting model and its deployment on AKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before start\n",
    "\n",
    "Install FTK using [shell](https://azuremlftkrelease.blob.core.windows.net/latest/install_amlpf_linux.sh) or [batch](https://azuremlftkrelease.blob.core.windows.net/latest/install_amlpf_windows.bat) scripts.  \n",
    "To run this notebook please install the python SDK by running \n",
    "```\n",
    "activate azuremlftk_nov2018\n",
    "pip install --upgrade azureml-sdk[notebooks,automl]\n",
    "```\n",
    "Login to Azure\n",
    "```\n",
    "az login\n",
    "```\n",
    "After installation is complete, select Kernel>Change Kernel>azuremlftk_nov2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from ftk import TimeSeriesDataFrame, ForecastDataFrame\n",
    "from ftk.operationalization import ScoreContext\n",
    "from ftk.transforms import TimeSeriesImputer, TimeIndexFeaturizer, DropColumns, GrainIndexFeaturizer \n",
    "from ftk.models import RegressionForecaster\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from ftk.pipeline import AzureMLForecastPipeline\n",
    "from ftk.data import load_dominicks_oj_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "To train and test model load the Dominicks data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tsdf, test_tsdf = load_dominicks_oj_dataset()\n",
    "# Use a TimeSeriesImputer to linearly interpolate missing values\n",
    "imputer = TimeSeriesImputer(input_column='Quantity', \n",
    "                            option='interpolate',\n",
    "                            method='linear',\n",
    "                            freq='W-WED')\n",
    "\n",
    "train_imputed_tsdf = imputer.transform(train_tsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the pipeline.\n",
    "Create the forecasting pipeline to be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_series_freq = 'W-WED'\n",
    "oj_series_seasonality = 52\n",
    "\n",
    "# DropColumns: Drop columns that should not be included for modeling. `logmove` is the log of the number of \n",
    "# units sold, so providing this number would be cheating. `WeekFirstDay` would be \n",
    "# redundant since we already have a feature for the last day of the week.\n",
    "columns_to_drop = ['logmove', 'WeekFirstDay', 'week']\n",
    "column_dropper = DropColumns(columns_to_drop)\n",
    "# TimeSeriesImputer: Fill missing values in the features\n",
    "# First, we need to create a dictionary with key as column names and value as values used to fill missing \n",
    "# values for that column. We are going to use the mean to fill missing values for each column.\n",
    "columns_with_missing_values = train_imputed_tsdf.columns[pd.DataFrame(train_imputed_tsdf).isnull().any()].tolist()\n",
    "columns_with_missing_values = [c for c in columns_with_missing_values if c not in columns_to_drop]\n",
    "missing_value_imputation_dictionary = {}\n",
    "for c in columns_with_missing_values:\n",
    "    missing_value_imputation_dictionary[c] = train_imputed_tsdf[c].mean()\n",
    "fillna_imputer = TimeSeriesImputer(option='fillna', \n",
    "                                   input_column=columns_with_missing_values,\n",
    "                                   value=missing_value_imputation_dictionary)\n",
    "# TimeIndexFeaturizer: extract temporal features from timestamps\n",
    "time_index_featurizer = TimeIndexFeaturizer(correlation_cutoff=0.1, overwrite_columns=True)\n",
    "\n",
    "# GrainIndexFeaturizer: create indicator variables for stores and brands\n",
    "grain_featurizer = GrainIndexFeaturizer(overwrite_columns=True, ts_frequency=oj_series_freq)\n",
    "\n",
    "random_forest_model_deploy = RegressionForecaster(estimator=RandomForestRegressor(), make_grain_features=False)\n",
    "\n",
    "pipeline_deploy = AzureMLForecastPipeline([('drop_columns', column_dropper), \n",
    "                                           ('fillna_imputer', fillna_imputer),\n",
    "                                           ('time_index_featurizer', time_index_featurizer),\n",
    "                                           ('random_forest_estimator', random_forest_model_deploy)\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the required files.\n",
    "We will now deploy the model as a web service. That means we will create a docker image with the service logic and deploy it as [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/en-us/services/kubernetes-service/). The image creation of Forecasting model requires the model contained in the pickle file and dependencies file. This file is required to create the conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline_deploy, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conda dependencies file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "################################################################################\n",
    "#\n",
    "# Create Azure ML Forecasting Toolkit Conda environments on Linux platforms. \n",
    "# This yml is used specifically in creating containers on ACR for use \n",
    "# AML deployments.\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "name: azuremlftk_nov2018\n",
    "dependencies:\n",
    "  # AzureML FTK dependencies\n",
    "  - pyodbc\n",
    "  - statsmodels\n",
    "  - pandas\n",
    "  - scikit-learn==0.19.1\n",
    "  - tensorflow\n",
    "  - keras\n",
    "  - distributed==1.23.1\n",
    "\n",
    "  - pip:\n",
    "    # AML logging\n",
    "    - https://azuremldownloads.azureedge.net/history-packages/preview/azureml.primitives-1.0.11.491405-py3-none-any.whl\n",
    "    - https://azuremldownloads.azureedge.net/history-packages/preview/azureml.logging-1.0.81-py3-none-any.whl\n",
    "    \n",
    "    #azure ml\n",
    "    - azureml-sdk[automl]\n",
    "    \n",
    "    #Dependencies from other AML packages\n",
    "    - https://azuremlftkrelease.blob.core.windows.net/azpkgdaily/azpkgcore-1.0.18309.1b1-py3-none-any.whl\n",
    "    - https://azuremlftkrelease.blob.core.windows.net/azpkgdaily/azpkgsql-1.0.18309.1b1-py3-none-any.whl\n",
    "\n",
    "    # AMLPF package  \n",
    "    - https://azuremlftkrelease.blob.core.windows.net/dailyrelease/azuremlftk-0.1.18305.1a1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a workspace object from persisted configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workspace is an Azure resource that holds all of your models, docker images, and services created. It can be configured using the file in json format. The example of this file is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workspace_aks.json\n",
    "{\n",
    "    \"subscription_id\": \"<subscription id>\",\n",
    "    \"resource_group\": \"<resource group>\",\n",
    "    \"workspace_name\": \"<workspace name>\",\n",
    "    \"location\": \"<location>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the workspace is not already present create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.exceptions import ProjectSystemException\n",
    "ws = None\n",
    "try:\n",
    "    #Try to get the workspace if it exists.\n",
    "    ws = Workspace.from_config(\"workspace_aks.json\")\n",
    "except ProjectSystemException:\n",
    "    #If the workspace was not found, create it.\n",
    "    with open(\"workspace_aks.json\", 'r') as config:\n",
    "        ws_data = json.load(config)\n",
    "    ws = Workspace.create(name = ws_data[\"workspace_name\"],\n",
    "                          subscription_id = ws_data[\"subscription_id\"],\n",
    "                          resource_group = ws_data[\"resource_group\"],\n",
    "                          location = ws_data[\"location\"])\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add tags and descriptions to your models. The below call registers `pipeline.pkl` file as a model with the name `aksforecast` in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = \"pipeline.pkl\",\n",
    "                       model_name = \"aksforecast\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are versioned. If you call the register_model command many times with same model name, you will get multiple versions of the model with increasing version numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file"
    ]
   },
   "outputs": [],
   "source": [
    "regression_models = Model.list(ws)\n",
    "for m in regression_models:\n",
    "    print(\"Name:\", m.name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick a specific model to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.name, model.description, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `score.py`. Note that the `aksforecast` in the `get_model_path` call is referring to a same named model `aksforecast` registered under the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from ftk.operationalization.score_script_helper import run_impl\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    #init method will be executed once at start of the docker - load the model\n",
    "    global pipeline\n",
    "    #Get the model path.\n",
    "    pipeline_pickle_file = Model.get_model_path(\"aksforecast\")\n",
    "    #Load the model.\n",
    "    with open(pipeline_pickle_file, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "\n",
    "#Run method is executed once per call.\n",
    "def run(input_data):\n",
    "    #The JSON encoded input_data will be interpreted as a TimeSeriedData frame and will \n",
    "    #be used for forecasting.\n",
    "    #Return the JSON encoded data frame with forecast.\n",
    "    return run_impl(input_data, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that following command can take a few minutes. An image can contain multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create image"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(runtime= \"python\",\n",
    "                                 execution_script=\"score.py\",\n",
    "                                 conda_file=\"conda_dependencies.yml\")\n",
    "\n",
    "image = Image.create(name = \"ftkimage1\",\n",
    "                     # this is the model object \n",
    "                     models = [model],\n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor image creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create image"
    ]
   },
   "outputs": [],
   "source": [
    "image.wait_for_creation(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List images and find out the detailed build log for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create image"
    ]
   },
   "outputs": [],
   "source": [
    "for i in Image.list(workspace = ws):\n",
    "    print('{}(v.{} [{}]) stored at {} with build log {}'.format(i.name, i.version, i.creation_state, i.image_location, i.image_build_log_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy image as web service on Azure Container Instance\n",
    "\n",
    "Create AKS using provisioning configuration. It defines the number of nodes, their sizes and SSL certificate for secure connection to the endpoint. Here we use the defaiult values for these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "# Use the default configuration (can also provide parameters to customize)\n",
    "prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "aks_name = 'my-aks-8' \n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                  name = aks_name, \n",
    "                                  provisioning_configuration = prov_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the AKS creation. This may take 15-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment configuration defines how much resources should be reserved for this container. Start deployment using newly created AKS. Note that the service creation can take few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "#Set the web service configuration (using default here)\n",
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "aks_service = Webservice.deploy_from_image(workspace = ws, \n",
    "                                           name = aks_name,\n",
    "                                           image = image,\n",
    "                                           deployment_config = aks_config,\n",
    "                                           deployment_target = aks_target)\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there was a problem during deployment it may be useful to analyze the deployment logs.\n",
    "Even after deployment operation is complete the container still may be in the \"Creating\" state. Make sure that creating is complete and logs are shown correctly before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aks_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a validation data set to benchmark new service.\n",
    "You might ask why we are sending a ForecastDataFrame to the service? We do it to give it the values of the future predictor variable, like price at the future time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = TimeSeriesImputer(input_column='Quantity', \n",
    "                            option='interpolate',\n",
    "                            method='linear',\n",
    "                            freq='W-WED')    \n",
    "train_imputed_tsdf = imputer.transform(train_tsdf)\n",
    "validate_ts = train_imputed_tsdf.assign(PointForecast=0.0, DistributionForecast=np.nan)\n",
    "validate_fdf = ForecastDataFrame(validate_ts, pred_point='PointForecast', pred_dist='DistributionForecast')\n",
    "sc_validate = ScoreContext(input_training_data_tsdf=train_imputed_tsdf,\n",
    "                           input_scoring_data_fcdf=validate_fdf, \n",
    "                           pipeline_execution_type='train_predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are sending the training data set to train the pickled model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed_tsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ForecastDataFrame for validation contains predictor values and the empty columns for predicted values. In this case it is columns DistributionForecast and PointForecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_fdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScoreContext contains both training and prediction(validation) data frames and helps to serialize these data to JSON format understood by the service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction and show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_direct =aks_service.run(sc_validate.to_json())\n",
    "fcdf_direct=ForecastDataFrame.construct_from_json(json_direct)\n",
    "fcdf_direct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete AKS service and resiurce group\n",
    "\n",
    "This part of a notebook is oprtional and intended to clean up after work is complete. First delete the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "aks_service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if services are present in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[svc.name for svc in Webservice.list(ws)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the resource group.<br/>\n",
    "**Note** This operation is danger and will delete all the content of the resource group.\n",
    "To delete group the azure sdk package needs to be installed:\n",
    "```\n",
    "pip install https://azuremlftkrelease.blob.core.windows.net/azpkgdaily/azpkgamlsdk-1.0.18309.1b1-py3-none-any.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azpkgamlsdk.deployment.utils_environment import delete_resource_group\n",
    "\n",
    "delete_resource_group(ws.resource_group, ws.subscription_id)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:azuremlftk_oct2018_test]",
   "language": "python",
   "name": "conda-env-azuremlftk_oct2018_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
