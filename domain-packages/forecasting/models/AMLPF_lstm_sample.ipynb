{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating LSTM model with Azure Machine Learning Package for Forecasting \n",
    "\n",
    "In this notebook, learn how to integrate LSTM model in the framework provided by Azure Machine Learning Package for Forecasting (AMLPF) to quickly build a forecasting model. \n",
    "We will use dow jones dataset to build a model that forecasts quarterly revenue for these 30 dow jones listed companies.\n",
    "\n",
    "#### Disclaimer: \n",
    "This notebook is based on the ongoing development work as part of the future release of AMLPF. Therefore, please consider this as a preview of what might become available in future as part of AMLPF. \n",
    "Further, please note that this work has currently been tested only on Windows platform.\n",
    "\n",
    "### Prerequisites:\n",
    "If you don't have an Azure subscription, create a free account before you begin. The following accounts and application must be set up and installed:<br/>\n",
    "* Azure Machine Learning Experimentation account.\n",
    "\n",
    "If these three are not yet created or installed, follow the Azure Machine Learning Quickstart and Workbench installation article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # comment out this statement if you do not want to suppress the warnings.\n",
    "\n",
    "import sys, os, inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import ftk\n",
    "ftk_root_path = (ftk.__path__)[0] # This is the path where ftk package is installed.\n",
    "\n",
    "from ftk.pipeline import AzureMLForecastPipeline\n",
    "\n",
    "from ftk.operationalization.dnnscorecontext import DnnScoreContext\n",
    "from ftk.operationalization.dnn_score_script_helper import score_run\n",
    "\n",
    "from ftk.dnn_utils import create_lag_lead_features\n",
    "from ftk.dnn_utils import pickle_keras_models\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import load_model\n",
    "\n",
    "print('imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000) # Set random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = ftk_root_path + \"\\\\data\\\\dow_jones\\\\dow_jones_data.tsv\" # Change it depending upon where this file is stored.\n",
    "num_lag_feats = 16 # Number of lag features to be used while training the model.\n",
    "num_leads = 0 # Lead zero indicates current-time's value. forecast only one step at a time. \n",
    "# Note: MAPE error computation is done considering num_leads = 0. It may need to be updated to take into account num_leads > 0. It has not been done yet.\n",
    "num_test_records = 4 # Keep last four records for each company in the test data.\n",
    "num_lstm_au = 50 # Number of units in single lstm layer.\n",
    "num_epochs = 150 # Number of epochs to fit the model. \n",
    "dj_series_freq = 'Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  quarter_start company_ticker  revenue\n",
      "0    2000-01-01           AAPL  1945.00\n",
      "1    2000-04-01           AAPL  1825.00\n",
      "2    2000-07-01           AAPL  1870.00\n",
      "3    2000-10-01           AAPL  1007.00\n",
      "4    2001-01-01           AAPL  1431.00\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1860 entries, 0 to 1859\n",
      "Data columns (total 3 columns):\n",
      "quarter_start     1860 non-null object\n",
      "company_ticker    1860 non-null object\n",
      "revenue           1835 non-null float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 43.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read the dow_jones_data.\n",
    "dj_df = pd.read_table(data_file_path)\n",
    "print(dj_df.head())\n",
    "print(dj_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1798 entries, 0 to 1859\n",
      "Data columns (total 3 columns):\n",
      "quarter_start     1798 non-null datetime64[ns]\n",
      "company_ticker    1798 non-null object\n",
      "revenue           1798 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 56.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Revenue has null values for some company. 'V' has been such identified company.\n",
    "# In this experiment, we remove the company from the dataset instead of interpolating.\n",
    "dj_df = dj_df[dj_df['company_ticker'] != 'V'] \n",
    "# Convert quarter_start field to datetime.\n",
    "dj_df['quarter_start'] = pd.to_datetime(dj_df['quarter_start'])\n",
    "print(dj_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by company to normalize it accordingly.\n",
    "grouped_data = dj_df.groupby(by='company_ticker')\n",
    "cmp_to_scaler = {}\n",
    "norm_dj_df = pd.DataFrame(columns=dj_df.columns) # Dataframe with quarter_start, company_ticker, normalized-revenue information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each company's data individually and save the scaler into a dictionary to be used later.\n",
    "for grp_name, grp_data in grouped_data:\n",
    "    cur_grp_data = grp_data.sort_values(by=['quarter_start'])\n",
    "    cur_grp_data = cur_grp_data.drop(['company_ticker', 'quarter_start'], axis=1)\n",
    "    scaler = MinMaxScaler(feature_range=(0.000001, 1)) \n",
    "    norm_grp_data = scaler.fit_transform(cur_grp_data)    \n",
    "    cmp_to_scaler[grp_name] = scaler\n",
    "    norm_grp_df = pd.DataFrame(norm_grp_data, columns=['revenue'])\n",
    "    aux_data_df = grp_data.loc[:,('quarter_start', 'company_ticker')]\n",
    "    aux_data_df.reset_index(drop=True, inplace=True)\n",
    "    cur_grp_norm_df = pd.concat((aux_data_df, norm_grp_df), axis=1)\n",
    "    norm_dj_df = norm_dj_df.append(cur_grp_norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 16 lags as features for each quarterly data point (normalized revenue in previous step).\n",
    "dj_reg = pd.DataFrame()\n",
    "norm_grp_data = norm_dj_df.groupby(by='company_ticker')\n",
    "for grp_name, grp_data in norm_grp_data:\n",
    "    cur_grp_data = grp_data.sort_values(by=['quarter_start'])\n",
    "    dj_reg_grp = create_lag_lead_features(cur_grp_data, ts_col='revenue', \n",
    "                    aux_cols=['company_ticker', 'quarter_start'], num_lags=num_lag_feats)\n",
    "    dj_reg = dj_reg.append(dj_reg_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of feature column-names.\n",
    "feat_cols = []\n",
    "feat_tgt_cols = []\n",
    "for i in range(num_lag_feats, 0, -1) :\n",
    "    feat_cols.append('revenueLag' + str(i))\n",
    "feat_tgt_cols.extend(feat_cols)\n",
    "\n",
    "# Create list of target column-names. \n",
    "target_cols = ['revenueLead0']\n",
    "for i in range(1, num_leads+1) :\n",
    "    target_cols.append('revenueLead' + str(i))\n",
    "feat_tgt_cols.extend(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into taining and test dataset for each company.\n",
    "dj_reg_grp_data = dj_reg.groupby(by='company_ticker')\n",
    "train_data = pd.DataFrame(columns=dj_reg.columns)\n",
    "test_data = pd.DataFrame(columns=dj_reg.columns)\n",
    "\n",
    "for grp_name, grp_data in dj_reg_grp_data:\n",
    "    cur_grp_data = grp_data.sort_values(by=['quarter_start'])\n",
    "    num_records = cur_grp_data.shape[0]\n",
    "    train_data = train_data.append(pd.DataFrame(cur_grp_data.iloc[:(num_records - num_test_records),:]))\n",
    "    test_data = test_data.append(pd.DataFrame(cur_grp_data.iloc[(num_records - num_test_records):,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 1, 16)\n",
      "(1218, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and target values for training data.\n",
    "train_X = train_data[feat_cols] \n",
    "train_Y = train_data[target_cols]\n",
    "\"\"\"\n",
    "Formatting the input to be of the shape (number_of_samples, timesteps, number_of_features). \n",
    "For detail explanation refer to https://keras.io/layers/recurrent/.\n",
    "Note: I am considering here single timestep (set to 1) and number of features to be 16. It could be specified in \n",
    "a different way (I mean, 16 timesteps instead of 1) and I plan to experiment that in future.\n",
    "\"\"\"\n",
    "train_X = train_X.values.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "train_Y = train_Y.values.reshape((train_Y.shape[0], train_Y.shape[1]))\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LSTM network.\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_lstm_au, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1)) #dimension of the output vector \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " - 3s - loss: 0.0180\n",
      "Epoch 2/150\n",
      " - 2s - loss: 0.0130\n",
      "Epoch 3/150\n",
      " - 2s - loss: 0.0119\n",
      "Epoch 4/150\n",
      " - 2s - loss: 0.0114\n",
      "Epoch 5/150\n",
      " - 2s - loss: 0.0111\n",
      "Epoch 6/150\n",
      " - 2s - loss: 0.0108\n",
      "Epoch 7/150\n",
      " - 2s - loss: 0.0106\n",
      "Epoch 8/150\n",
      " - 2s - loss: 0.0105\n",
      "Epoch 9/150\n",
      " - 2s - loss: 0.0104\n",
      "Epoch 10/150\n",
      " - 2s - loss: 0.0102\n",
      "Epoch 11/150\n",
      " - 2s - loss: 0.0102\n",
      "Epoch 12/150\n",
      " - 2s - loss: 0.0101\n",
      "Epoch 13/150\n",
      " - 2s - loss: 0.0100\n",
      "Epoch 14/150\n",
      " - 2s - loss: 0.0099\n",
      "Epoch 15/150\n",
      " - 2s - loss: 0.0099\n",
      "Epoch 16/150\n",
      " - 2s - loss: 0.0098\n",
      "Epoch 17/150\n",
      " - 2s - loss: 0.0097\n",
      "Epoch 18/150\n",
      " - 2s - loss: 0.0097\n",
      "Epoch 19/150\n",
      " - 2s - loss: 0.0096\n",
      "Epoch 20/150\n",
      " - 2s - loss: 0.0096\n",
      "Epoch 21/150\n",
      " - 2s - loss: 0.0095\n",
      "Epoch 22/150\n",
      " - 2s - loss: 0.0095\n",
      "Epoch 23/150\n",
      " - 2s - loss: 0.0094\n",
      "Epoch 24/150\n",
      " - 2s - loss: 0.0094\n",
      "Epoch 25/150\n",
      " - 2s - loss: 0.0093\n",
      "Epoch 26/150\n",
      " - 2s - loss: 0.0093\n",
      "Epoch 27/150\n",
      " - 2s - loss: 0.0093\n",
      "Epoch 28/150\n",
      " - 2s - loss: 0.0092\n",
      "Epoch 29/150\n",
      " - 2s - loss: 0.0092\n",
      "Epoch 30/150\n",
      " - 2s - loss: 0.0091\n",
      "Epoch 31/150\n",
      " - 2s - loss: 0.0091\n",
      "Epoch 32/150\n",
      " - 2s - loss: 0.0091\n",
      "Epoch 33/150\n",
      " - 2s - loss: 0.0090\n",
      "Epoch 34/150\n",
      " - 2s - loss: 0.0090\n",
      "Epoch 35/150\n",
      " - 2s - loss: 0.0090\n",
      "Epoch 36/150\n",
      " - 2s - loss: 0.0089\n",
      "Epoch 37/150\n",
      " - 2s - loss: 0.0089\n",
      "Epoch 38/150\n",
      " - 2s - loss: 0.0089\n",
      "Epoch 39/150\n",
      " - 2s - loss: 0.0088\n",
      "Epoch 40/150\n",
      " - 2s - loss: 0.0088\n",
      "Epoch 41/150\n",
      " - 2s - loss: 0.0088\n",
      "Epoch 42/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 43/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 44/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 45/150\n",
      " - 2s - loss: 0.0087\n",
      "Epoch 46/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 47/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 48/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 49/150\n",
      " - 2s - loss: 0.0086\n",
      "Epoch 50/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 51/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 52/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 53/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 54/150\n",
      " - 2s - loss: 0.0085\n",
      "Epoch 55/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 56/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 57/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 58/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 59/150\n",
      " - 2s - loss: 0.0084\n",
      "Epoch 60/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 61/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 62/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 63/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 64/150\n",
      " - 2s - loss: 0.0083\n",
      "Epoch 65/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 66/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 67/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 68/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 69/150\n",
      " - 2s - loss: 0.0082\n",
      "Epoch 70/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 71/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 72/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 73/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 74/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 75/150\n",
      " - 2s - loss: 0.0081\n",
      "Epoch 76/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 77/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 78/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 79/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 80/150\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 81/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 82/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 83/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 84/150\n",
      " - 2s - loss: 0.0079\n",
      "Epoch 85/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 86/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 87/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 88/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 89/150\n",
      " - 2s - loss: 0.0078\n",
      "Epoch 90/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 91/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 92/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 93/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 94/150\n",
      " - 2s - loss: 0.0077\n",
      "Epoch 95/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 96/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 97/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 98/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 99/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 100/150\n",
      " - 2s - loss: 0.0076\n",
      "Epoch 101/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 102/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 103/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 104/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 105/150\n",
      " - 2s - loss: 0.0075\n",
      "Epoch 106/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 107/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 108/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 109/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 110/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 111/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 112/150\n",
      " - 2s - loss: 0.0074\n",
      "Epoch 113/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 114/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 115/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 116/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 117/150\n",
      " - 2s - loss: 0.0073\n",
      "Epoch 118/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 119/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 120/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 121/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 122/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 123/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 124/150\n",
      " - 2s - loss: 0.0072\n",
      "Epoch 125/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 126/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 127/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 128/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 129/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 130/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 131/150\n",
      " - 2s - loss: 0.0071\n",
      "Epoch 132/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 133/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 134/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 135/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 136/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 137/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 138/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 139/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 140/150\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 141/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 142/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 143/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 144/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 145/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 146/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 147/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 148/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 149/150\n",
      " - 2s - loss: 0.0069\n",
      "Epoch 150/150\n",
      " - 2s - loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "# Fit network. Currently set the batch_size=1; will add more relevant information on this later.\n",
    "history = model.fit(train_X, train_Y, epochs=num_epochs, batch_size=1, verbose=2, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50)                13400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,451\n",
      "Trainable params: 13,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print model.summary.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_keras_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 2018-10-19 14:53:12,256 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:12,259 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002999\n",
      "[[0.5019349455833435], [0.8112455606460571], [0.7845430970191956], [0.6465986967086792]]\n",
      "Company: AAPL Test MAPE: 7.961\n",
      "F1 2018-10-19 14:53:12,513 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:12,515 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002002\n",
      "[[0.5705405473709106], [0.5733838677406311], [0.5841577053070068], [0.5440548658370972]]\n",
      "Company: AXP Test MAPE: 4.998\n",
      "F1 2018-10-19 14:53:12,759 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:12,762 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.003002\n",
      "[[0.7879495620727539], [0.9370035529136658], [0.7566028833389282], [0.8646280169487]]\n",
      "Company: BA Test MAPE: 6.665\n",
      "F1 2018-10-19 14:53:12,983 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:12,985 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001977\n",
      "[[0.6817680597305298], [0.7210550904273987], [0.6542812585830688], [0.6862397789955139]]\n",
      "Company: CAT Test MAPE: 3.790\n",
      "F1 2018-10-19 14:53:13,199 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:13,201 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001951\n",
      "[[0.8580275774002075], [0.8995929956436157], [0.8698850870132446], [0.8763518333435059]]\n",
      "Company: CSCO Test MAPE: 6.220\n",
      "F1 2018-10-19 14:53:13,424 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:13,427 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.003001\n",
      "[[0.6197220683097839], [0.58384108543396], [0.47058725357055664], [0.4055095314979553]]\n",
      "Company: CVX Test MAPE: 12.392\n",
      "F1 2018-10-19 14:53:13,638 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:13,640 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002001\n",
      "[[0.5509114265441895], [0.48586469888687134], [0.8292276859283447], [0.7129167318344116]]\n",
      "Company: DD Test MAPE: 2.300\n",
      "F1 2018-10-19 14:53:13,854 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:13,856 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001976\n",
      "[[0.8008443713188171], [0.8768565058708191], [0.8275035619735718], [0.9088221788406372]]\n",
      "Company: DIS Test MAPE: 4.319\n",
      "F1 2018-10-19 14:53:14,094 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:14,096 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002383\n",
      "[[0.46674424409866333], [0.6262599229812622], [0.5379558801651001], [0.3695982098579407]]\n",
      "Company: GE Test MAPE: 9.797\n",
      "F1 2018-10-19 14:53:14,305 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:14,307 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001998\n",
      "[[0.6391887664794922], [0.6984357237815857], [0.675517737865448], [0.6930989027023315]]\n",
      "Company: GS Test MAPE: 9.544\n",
      "F1 2018-10-19 14:53:14,519 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:14,521 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001976\n",
      "[[0.9166229367256165], [0.778295636177063], [0.6803483963012695], [0.7301689386367798]]\n",
      "Company: HD Test MAPE: 2.613\n",
      "F1 2018-10-19 14:53:14,743 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:14,745 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002002\n",
      "[[0.4658582806587219], [0.7373098134994507], [0.26033490896224976], [0.38567522168159485]]\n",
      "Company: IBM Test MAPE: 7.313\n",
      "F1 2018-10-19 14:53:14,967 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:14,969 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001998\n",
      "[[0.8773013353347778], [0.9023150205612183], [0.8526875972747803], [0.7932122349739075]]\n",
      "Company: INTC Test MAPE: 4.652\n",
      "F1 2018-10-19 14:53:15,181 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:15,184 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002999\n",
      "[[0.9169779419898987], [0.8884966373443604], [0.8440040349960327], [0.842350959777832]]\n",
      "Company: JNJ Test MAPE: 0.935\n",
      "F1 2018-10-19 14:53:15,398 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:15,400 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002002\n",
      "[[0.7961400151252747], [0.7638212442398071], [0.7560436725616455], [0.7890310287475586]]\n",
      "Company: JPM Test MAPE: 3.321\n",
      "F1 2018-10-19 14:53:15,617 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:15,619 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002002\n",
      "[[0.8228322267532349], [0.7649111151695251], [0.6946200132369995], [0.8252671957015991]]\n",
      "Company: KO Test MAPE: 3.440\n",
      "F1 2018-10-19 14:53:15,830 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:15,832 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001997\n",
      "[[0.9184539318084717], [0.8226886987686157], [0.7199496626853943], [0.6964280605316162]]\n",
      "Company: MCD Test MAPE: 2.734\n",
      "F1 2018-10-19 14:53:16,040 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:16,042 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.003002\n",
      "[[0.9191059470176697], [0.8758713603019714], [0.8797422647476196], [0.8456687927246094]]\n",
      "Company: MMM Test MAPE: 2.259\n",
      "F1 2018-10-19 14:53:16,253 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:16,255 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001975\n",
      "[[0.8432249426841736], [0.8442872762680054], [0.819187343120575], [0.818576455116272]]\n",
      "Company: MRK Test MAPE: 17.530\n",
      "F1 2018-10-19 14:53:16,468 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:16,470 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002001\n",
      "[[0.7611593008041382], [0.9773409366607666], [0.8092085123062134], [0.7850304841995239]]\n",
      "Company: MSFT Test MAPE: 3.373\n",
      "F1 2018-10-19 14:53:16,687 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:16,689 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001999\n",
      "[[0.8868825435638428], [0.8299511671066284], [0.8612222075462341], [0.8935779333114624]]\n",
      "Company: NKE Test MAPE: 5.671\n",
      "F1 2018-10-19 14:53:16,910 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:16,912 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.001998\n",
      "[[0.5803912878036499], [0.5929268598556519], [0.5137486457824707], [0.477559894323349]]\n",
      "Company: PFE Test MAPE: 4.288\n",
      "F1 2018-10-19 14:53:17,123 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:17,125 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002000\n",
      "[[0.9437791109085083], [0.8488930463790894], [0.7284170389175415], [0.6994599103927612]]\n",
      "Company: PG Test MAPE: 2.563\n",
      "F1 2018-10-19 14:53:17,347 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:17,350 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002980\n",
      "[[0.8873205184936523], [0.9141828417778015], [0.8851954936981201], [0.8846676349639893]]\n",
      "Company: TRV Test MAPE: 6.392\n",
      "F1 2018-10-19 14:53:17,557 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:17,559 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002003\n",
      "[[0.8364039659500122], [0.8534146547317505], [0.8760096430778503], [0.9121664762496948]]\n",
      "Company: UNH Test MAPE: 6.751\n",
      "F1 2018-10-19 14:53:17,779 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:17,781 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002001\n",
      "[[0.9345104098320007], [0.9162583351135254], [0.7824671864509583], [0.8453519344329834]]\n",
      "Company: UTX Test MAPE: 3.282\n",
      "F1 2018-10-19 14:53:17,994 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:17,997 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002978\n",
      "[[0.8728854656219482], [0.8866824507713318], [0.9081341624259949], [0.8893343806266785]]\n",
      "Company: VZ Test MAPE: 4.065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 2018-10-19 14:53:18,204 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:18,207 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.003002\n",
      "[[0.8005287051200867], [0.8174129724502563], [0.9056491851806641], [0.8258925080299377]]\n",
      "Company: WMT Test MAPE: 3.929\n",
      "F1 2018-10-19 14:53:18,412 INFO azureml.timeseries - pipeline predict started. \n",
      "F1 2018-10-19 14:53:18,414 INFO azureml.timeseries - pipeline predict finished. Time elapsed 0:00:00.002001\n",
      "[[0.6887446641921997], [0.6516265869140625], [0.454683393239975], [0.3722485601902008]]\n",
      "Company: XOM Test MAPE: 14.283\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataframe with column-names to hold forecasts and other relevant information.\n",
    "final_test_forecasts = pd.DataFrame(columns=['company_ticker', 'quarter_start', 'actual', 'forecast'])\n",
    "\n",
    "# Initialize dataframe with column-names to hold MAPE (Mean Absolute Percentage Error) for each company.\n",
    "final_mapes = pd.DataFrame(columns=['company_ticker', 'mape'])\n",
    "\n",
    "\"\"\"\n",
    "Compute prediction of test data one company at a time. \n",
    "This is to simplify the process of scaling it back to original scale for that company.\n",
    "\"\"\"\n",
    "test_grp_data = test_data.groupby(by='company_ticker')\n",
    "\n",
    "for grp_name, grp_data in test_grp_data:\n",
    "    cur_grp_data = grp_data.reset_index(drop=True)\n",
    "    cur_grp_data['quarter_start'] = pd.to_datetime(cur_grp_data['quarter_start'])\n",
    "    cur_grp_data = cur_grp_data.sort_values(by=['quarter_start'])\n",
    "    cur_final_test_fcasts = cur_grp_data[['company_ticker', 'quarter_start']]\n",
    "    scaler = cmp_to_scaler[grp_name]\n",
    "\n",
    "    test_X = cur_grp_data[feat_cols]\n",
    "    test_Y = cur_grp_data[target_cols]\n",
    "    test_X_reshape = test_X.values.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    \n",
    "    dnnscoreobject = DnnScoreContext(input_scoring_data=test_X_reshape, \n",
    "                                     pipeline_execution_type='predict') # construct a context object to be used for scoring purpose.\n",
    "    pipeline_lstm = AzureMLForecastPipeline([('lstm_model', model)])\n",
    "    #yhat = service.score(score_context=dnnscoreobject) # invoke the web service to get predictions on the test data.\n",
    "    yhat = json.loads(score_run(dnn_score_context=dnnscoreobject, pipeline=pipeline_lstm))\n",
    "    print(yhat)\n",
    "    inv_x_yhat = pd.concat((test_X, pd.DataFrame(yhat)), axis=1)   \n",
    "    inv_x_yhat = scaler.inverse_transform(inv_x_yhat)    \n",
    "    inv_x_yhat_df = pd.DataFrame(inv_x_yhat, columns=feat_tgt_cols)\n",
    "    inv_yhat = inv_x_yhat_df[target_cols] \n",
    "    cur_final_test_fcasts['forecast'] = inv_yhat\n",
    "        \n",
    "    inv_x_y = pd.concat((test_X, pd.DataFrame(test_Y)), axis=1)\n",
    "    inv_x_y = scaler.inverse_transform(inv_x_y)\n",
    "    inv_x_y_df = pd.DataFrame(inv_x_y, columns=feat_tgt_cols)\n",
    "    inv_y = inv_x_y_df[target_cols]\n",
    "    cur_final_test_fcasts['actual'] = inv_y\n",
    "\n",
    "    final_test_forecasts = final_test_forecasts.append(cur_final_test_fcasts)\n",
    "    mape = (np.mean(np.abs((inv_y - inv_yhat)/inv_y)))*100\n",
    "    print('Company: ' + grp_name + ' Test MAPE: %.3f' % mape)\n",
    "    final_mapes = final_mapes.append({'company_ticker' : grp_name, 'mape' : mape}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
