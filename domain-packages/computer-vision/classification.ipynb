{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning for Computer Vision Package \n",
    "\n",
    "A large number of problems in the computer vision domain can be solved using image classification approaches. These include building models which answer questions such as, \"Is an OBJECT present in the image?\" (where OBJECT could for example be \"dog\", \"car\", \"ship\", etc.) as well as more complex questions, like \"What class of eye disease severity is evinced by this patient's retinal scan?\"\n",
    "\n",
    "This notebook shows how the Azure Machine Learning for Computer Vision Package (CVTK) can be used to train, test, and deploy an **image classification** model. For more details, please check the API doc. Currently, [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/) is used as the deep learning framework, training is performed locally on a GPU powered machine such as the ([Data Science VM](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning?tab=Overview)), and deployment uses the Azure ML Operationalization CLI.\n",
    "\n",
    "The following steps are performed:\n",
    "1. Dataset Creation\n",
    "2. Image Visualization and annotation\n",
    "3. Image Augmentation\n",
    "4. DNN Model Definition\n",
    "5. Classifier Training\n",
    "6. Evaluation and Visualization\n",
    "7. Webservice Deployment\n",
    "8. Webservice Load Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset\n",
    "\n",
    "As running example we use a dataset consisting of 63 tableware images, each labeled as belonging to one of four different classes (bowl, cup, cutlery, plate). We kept the number of images small so that this tutorial can be executed quickly; in practice at least 100 images per class should be provided. All images are located at *\"../sample_data/imgs_recycling/\"*, in subdirectories called \"bowl\", \"cup\", \"cutlery\", and \"plate\".\n",
    "\n",
    "<img src=\"misc/recycling_examples.jpg\" bowl width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Context\n",
    "The storage context is used to determine where various output files such as augmented images or DNN model files will be stored (see the StorageContext documentation for more details). Normally, the storage content does not need to be set explicitely. However, when using the AML Workbench, to avoid its 25 MB limit on the project size, we are setting the cvtk outputs directory to point to a location outside the AML project (\"../../../../cvtk_output\"). Make sure to remove the \"cvtk_output\" directory once it is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json, numpy as np, os, timeit \n",
    "from azureml.logging import get_azureml_logger\n",
    "from imgaug import augmenters\n",
    "from IPython.display import display\n",
    "from sklearn import svm\n",
    "from cvtk import ClassificationDataset, CNTKTLModel, Context, Splitter, StorageContext\n",
    "from cvtk.augmentation import augment_dataset\n",
    "from cvtk.core.classifier import ScikitClassifier\n",
    "from cvtk.evaluation import ClassificationEvaluation, graph_roc_curve, graph_pr_curve, graph_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Disable printing of logging messages\n",
    "from azuremltkbase.logging import ToolkitLogger\n",
    "ToolkitLogger.getInstance().setEnabled(False)\n",
    "\n",
    "# Set storage context.\n",
    "out_root_path = \"../../../cvtk_output\"\n",
    "Context.create(outputs_path=out_root_path, persistent_path=out_root_path, temp_path=out_root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way to generate a Dataset object in CVTK is by providing the root directory of the images on the local disk. This directory has to follow the same general structure as the tableware dataset, ie. contain sub-directories with the actual images:\n",
    "- root\n",
    "    - label1\n",
    "    - label2\n",
    "    - ...\n",
    "    - labeln\n",
    "  \n",
    "Training an image classification model for a different dataset is therefore as easy as changing the root path `dataset_location` in the code below to point at different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root image directory \n",
    "dataset_location = os.path.abspath(os.path.join(os.getcwd(), \"../sample_data/imgs_recycling\"))\n",
    "\n",
    "dataset_name = 'recycling'\n",
    "dataset = ClassificationDataset.create_from_dir(dataset_name, dataset_location)\n",
    "print(\"Dataset consists of {} images with {} labels.\".format(len(dataset.images), len(dataset.labels)))\n",
    "print(\"Select information for image 2: name={}, label={}, unique id={}.\".format(\n",
    "    dataset.images[2].name, dataset.images[2]._labels[0].name, dataset.images[2]._storage_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Image visualization and annotation\n",
    "\n",
    "The widget below can be used to visualize the images in the dataset object, and to correct some of the labels if needed. In case you see a \"Widget Javascript not detected\" error, this can often be solved by running the command: \n",
    "  `jupyter nbextension enable --py --sys-prefix widgetsnbextension`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ui_utils.ui_annotation import AnnotationUI\n",
    "annotation_ui = AnnotationUI(dataset, Context.get_global_context())\n",
    "display(annotation_ui.ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Image Augmentation\n",
    "\n",
    "The augmentation module provides functionality to augment a dataset object using all the transformations described in the [imgaug](https://github.com/aleju/imgaug) library. Image transformations can be grouped in a single pipeline, in which case all transformations in the pipeline are applied simultaniously to each image. If you would like to apply different augmentation steps separately, or in any different manner, you can define multiple pipelines and pass them to the *augment_dataset* function. For more details and examples of image augmentation, see the [imgaug documentation](https://github.com/aleju/imgaug).\n",
    "\n",
    "Adding augmented images to the training set is especially beneficial for small datasets. However, since it slows down DNN training due to the increased number of training images, our recommendation is to start experimentation without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test  \n",
    "train_set_orig, test_set = dataset.split(train_size = 0.66, stratify = \"label\", random_state = 0)\n",
    "print(\"Number of training images = {}, test images = {}.\".format(train_set_orig.size(), test_set.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_train_set = False\n",
    "\n",
    "if augment_train_set:\n",
    "    aug_sequence = augmenters.Sequential([\n",
    "            augmenters.Fliplr(0.5),             # horizontally flip 50% of all images\n",
    "            augmenters.Crop(percent=(0, 0.1)),  # crop images by 0-10% of their height/width\n",
    "        ])\n",
    "    train_set = augment_dataset(train_set_orig, [aug_sequence])\n",
    "    print(\"Number of original training images = {}, with augmented images included = {}.\".format(train_set_orig.size(), train_set.size()))\n",
    "else:\n",
    "    train_set = train_set_orig  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: DNN Model Definition\n",
    "\n",
    "Six different per-trained Deep Neural Network models are supported in CVTK: AlexNet, Resnet-18, Resnet-34, and Resnet-50, Resnet-101, and Resnet-152. These DNNs can be used either as classifier, or as featurizer (see step 5). More information about the networks can be found [here](https://github.com/Microsoft/CNTK/blob/master/PretrainedModels/Image.md), and a basic introduction to Transfer Learning is [here](https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0).\n",
    "\n",
    "CVTK comes with default parametes (224x224 pixel resolution and Resnet-18 DNN) which were selected to work well on a wide variety of tasks. Accuracy can often be improved by eg. increasing the image resolution to 500x500 pixels, and/or selecting a deeper model (Resnet-50), however this comes at a significant increase in training time. See the \"How to improve accuracy\" section in the Appendix for more detail, and why the minibatch-size and the learning rate need to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVTK default parameters (224 x 224 pixels resolution, Resnet-18)\n",
    "lr_per_mb = [0.05]*7 + [0.005]*7 +  [0.0005]\n",
    "mb_size = 32\n",
    "input_resoluton = 224\n",
    "base_model_name = \"ResNet18_ImageNet_CNTK\"\n",
    "\n",
    "# CVTK suggested parameters for 500 x 500 pixels resolution, Resnet-50\n",
    "# (see in the Appendix \"How to improve accuracy\", last row in table)\n",
    "# lr_per_mb   = [0.01] * 7 + [0.001] * 7 + [0.0001]\n",
    "# mb_size    = 8\n",
    "# input_resoluton = 500\n",
    "# base_model_name = \"ResNet50_ImageNet_CNTK\"\n",
    "\n",
    "# Initialize model\n",
    "dnn_model = CNTKTLModel(train_set.labels,\n",
    "                       base_model_name=base_model_name,\n",
    "                       image_dims = (3, input_resoluton, input_resoluton))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Classifier Training\n",
    "\n",
    "The pre-trained DNN from the last section can be used in two ways:\n",
    "  - DNN refinement: This trains the DNN to directly perform the classification. While DNN training is slow, it typically leads to the best results since all network weights can be improved during training to give best accuracy.\n",
    "  - DNN featurization: This runs the DNN as-is to obtain a lower-dimensional representation of an image (512, 2048, or 4096 floats), which is then used as input to train a separate classifier. Since the DNN is kept unchanged, this approach is much faster compared to DNN refinement, however accuracy is not as good. Nevertheless, training an external classifier such as a linear SVM (shown below) can provide a strong baseline, and help with understanding the feasibility of a problem.\n",
    "  \n",
    "#### Tensorboard Logging\n",
    "Tensorboard can be used to visualize the training progress. To activate, \n",
    "1. Add the parameter `tensorboard_logdir=PATH` as shown below. \n",
    "2. Then start the tensorboard client by running in a new console the command `tensorboard --logdir=PATH` \n",
    "3. Open a web-browser as instructed by tensorboard (default is localhost:6006). \n",
    "\n",
    "`PATH` is the path to which you will log your files. Please note, you will need to have sufficient runs before tensorboard will start logging data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train either the DNN or a SVM as classifier \n",
    "classifier_name = \"dnn\"\n",
    "\n",
    "if classifier_name.lower() == \"dnn\":  \n",
    "    dnn_model.train(train_set, lr_per_mb = lr_per_mb, mb_size = mb_size, eval_dataset=test_set) #, tensorboard_logdir=r\"tensorboard\"\n",
    "    classifier = dnn_model\n",
    "# You can uncomment the following lines and change the classifier_name to \"svm\" to enable svm classifier. \n",
    "# However, deployment of SVM classifier is not supported now.\n",
    "# elif classifier_name.lower() == \"svm\":\n",
    "#     learner = svm.LinearSVC(C=1.0, class_weight='balanced', verbose=0)\n",
    "#     classifier = ScikitClassifier(dnn_model, learner = learner)\n",
    "#     classifier.train(train_set)\n",
    "else:\n",
    "    raise Exception(\"Classifier unknown: \" + classifier)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how the training and test accuracy increases during gradient descent. \n",
    "if classifier_name == \"dnn\":\n",
    "    [train_accs, test_accs, epoch_numbers] = classifier.train_eval_accs\n",
    "    plt.xlabel(\"Number of training epochs\") \n",
    "    plt.ylabel(\"Classification accuracy\") \n",
    "    train_plot = plt.plot(epoch_numbers, train_accs, 'r-', label = \"Training accuracy\")\n",
    "    test_plot = plt.plot(epoch_numbers, test_accs, 'b-.', label = \"Test accuracy\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation module provides functionality to evaluate the performance of the trained model on an independent test dataset. Some of the evaluation metrics it computes are: accuracy (by default class-averaged), PR curve, ROC curve, area-under-curve, confusion metrix, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on all test set images\n",
    "ce = ClassificationEvaluation(classifier, test_set, minibatch_size = mb_size)\n",
    "\n",
    "# Compute Accuracy and the confusion matrix\n",
    "acc = ce.compute_accuracy()\n",
    "print(\"Accuracy = {:2.2f}%\".format(100*acc))\n",
    "cm  = ce.compute_confusion_matrix()\n",
    "print(\"Confusion matrix = \\n{}\".format(cm))\n",
    "\n",
    "# Show PR curve, ROC curve, and confusion matrix\n",
    "fig, ((ax1, ax2, ax3)) = plt.subplots(1,3)\n",
    "fig.set_size_inches(18, 4)\n",
    "graph_roc_curve(ce, ax=ax1)\n",
    "graph_pr_curve(ce, ax=ax2)\n",
    "graph_confusion_matrix(ce, ax=ax3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results viewer UI\n",
    "labels = [l.name for l in dataset.labels] \n",
    "pred_scores = ce.scores #classification scores for all images and all classes\n",
    "pred_labels = [labels[i] for i in np.argmax(pred_scores, axis=1)]\n",
    "\n",
    "from ui_utils.ui_results_viewer import ResultsUI\n",
    "results_ui = ResultsUI(test_set, Context.get_global_context(), pred_scores, pred_labels)\n",
    "display(results_ui.ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision / recall curve UI\n",
    "precisions, recalls, thresholds = ce.compute_precision_recall_curve() \n",
    "thresholds = list(thresholds)\n",
    "thresholds.append(thresholds[-1])\n",
    "from ui_utils.ui_precision_recall import PrecisionRecallUI\n",
    "pr_ui = PrecisionRecallUI(100*precisions[::-1], 100*recalls[::-1], thresholds[::-1])\n",
    "display(pr_ui.ui) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Webservice Deployment\n",
    "\n",
    "\n",
    "<b>Prerequisites:</b> \n",
    "Please the check the **Prerequisites** section of our deployment notebook to set up your deployment CLI. You only need to set it up once for all your deployments. More deployment related topics including IoT Edge deployment can be found in the deployment notebook.\n",
    "       \n",
    "<b>Deployment API:</b>\n",
    "\n",
    "> **Examples:**\n",
    "- ```deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=dnn_model, aml_env=\"cluster\")``` # create deployment object\n",
    "- ```deploy_obj.deploy()``` # deploy web service\n",
    "- ```deploy_obj.status()``` # get status of deployment\n",
    "- ```deploy_obj.score_image(local_image_path_or_image_url)``` # score an image\n",
    "- ```deploy_obj.delete()``` # delete the web service\n",
    "- ```deploy_obj.build_docker_image()``` # build docker image without creating webservice\n",
    "- ```AMLDeployment.list_deployment()``` # list existing deployment\n",
    "- ```AMLDeployment.delete_if_service_exist(deployment_name)``` # delete if the service exists with the deployment name\n",
    "\n",
    "<b>Deployment management with portal:</b>\n",
    "\n",
    "You can go to [Azure portal](https://ms.portal.azure.com/) to track and manage your deployments. From Azure portal, find your Machine Learning Model Management account page (You can search for your model management account name). Then go to: the model management account page->Model Management->Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### OPTIONAL - Interactive CLI setup helper ###### \n",
    "# # Interactive CLI setup helper, including model management account and deployment environment.\n",
    "# # If you haven't setup you CLI before or if you want to change you CLI settings, you can use this block to help you interactively.\n",
    "# # UNCOMMENT THE FOLLOWING LINES IF YOU HAVE NOT CREATED OR SET THE MODEL MANAGEMENT ACCOUNT AND DEPLOYMENT ENVIRONMENT\n",
    "\n",
    "# from azuremltkbase.deployment import CliSetup\n",
    "# CliSetup().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional. Persist you model on disk and reuse it later for deployment. \n",
    "# from cvtk import CNTKTLModel, Context\n",
    "# import os\n",
    "# save_model_path = os.path.join(Context.get_global_context().storage.persistent_path, \"saved_classifier.model\")\n",
    "# # Save model to disk\n",
    "# dnn_model.save(save_model_path)\n",
    "# # Load model from disk\n",
    "# dnn_model = CNTKTLModel.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "# set deployment name\n",
    "deployment_name = \"icdeployment\"\n",
    "\n",
    "# Create deployment object\n",
    "# It will use the current deployment environment (you can check it with CLI command \"az ml env show\").\n",
    "deploy_obj = AMLDeployment(deployment_name=deployment_name, aml_env=\"cluster\", associated_DNNModel=dnn_model, replicas=1)\n",
    "\n",
    "# Alternatively, you can provide azure machine learning deployment cluster name (environment name) and resource group name\n",
    "# to deploy your model. It will use the provided cluster to deploy. To do that, please uncomment the following lines to create \n",
    "# the deployment object.\n",
    "\n",
    "# azureml_rscgroup = \"<resource group>\"\n",
    "# cluster_name = \"<cluster name>\"\n",
    "# deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=dnn_model,\n",
    "#                            aml_env=\"cluster\", cluster_name=cluster_name, resource_group=azureml_rscgroup, replicas=1)\n",
    "\n",
    "# Check if the deployment name exists, if yes remove it first. \n",
    "# Note: This will delete existing webservice with the same name. Do not delete the webservice unintentionally. \n",
    "if deploy_obj.is_existing_service():\n",
    "    AMLDeployment.delete_if_service_exist(deployment_name)\n",
    "    \n",
    "# create the webservice\n",
    "print(\"Deploying to Azure cluster...\")\n",
    "deploy_obj.deploy()\n",
    "print(\"Deployment DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Webservice comsumption\n",
    "\n",
    "Once you created the webservice, you can score images with the deployed webservice. You have several options:\n",
    "\n",
    "   - You can directly score the webservice with the deployment object with: deploy_obj.score_image(image_path_or_url) \n",
    "   - Or, you can use the Service endpoin url and Serivce key (None for local deployment) with: AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)\n",
    "   - Form your http requests directly to score the webservice endpoint (For advanced users)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with existing deployment object\n",
    "```\n",
    "deploy_obj.score_image(image_path_or_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with existing deployment object\n",
    "\n",
    "# Score local image with file path\n",
    "print(\"Score local image with file path\")\n",
    "image_path_or_url = test_set.images[0].storage_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)\n",
    "\n",
    "# Score image url and remove image resizing\n",
    "print(\"Score image url\")\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)\n",
    "\n",
    "# Score image url with added paramters. Add softmax to score.\n",
    "print(\"Score image url with added paramters. Add softmax to score\")\n",
    "from cvtk.utils.constants import ClassificationRESTApiParamters\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url, image_resize_dims=[224,224], parameters={ClassificationRESTApiParamters.ADD_SOFTMAX:True})\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time image scoring\n",
    "import timeit\n",
    "\n",
    "num_images = 10\n",
    "for img_index, img_obj in enumerate(test_set.images[:num_images]):\n",
    "    print(\"Calling API for image {} of {}: {}...\".format(img_index, num_images, img_obj.name))\n",
    "    tic = timeit.default_timer()\n",
    "    return_json = deploy_obj.score_image(img_obj.storage_path, image_resize_dims=[224,224])\n",
    "    print(\"   Time for API call: {:.2f} seconds\".format(timeit.default_timer() - tic))\n",
    "    print(return_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with service endpoint url and service key\n",
    "`AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related classes and functions\n",
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "service_endpoint_url = \"\" # please replace with your own service url\n",
    "service_key = \"\" # please replace with your own service key\n",
    "# score local image with file path\n",
    "image_path_or_url = test_set.images[0].storage_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = AMLDeployment.score_existing_service_with_image(image_path_or_url,service_endpoint_url, service_key = service_key)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)\n",
    "\n",
    "# score image url\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = AMLDeployment.score_existing_service_with_image(image_path_or_url,service_endpoint_url, service_key = service_key, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score endpoint with http request directly\n",
    "Following is some example code to form the http request directly in Python. You can do it in other programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_image_with_http(image, service_endpoint_url, service_key=None, parameters={}):\n",
    "    \"\"\"Score local image with http request\n",
    "\n",
    "    Args:\n",
    "        image (str): Image file path\n",
    "        service_endpoint_url(str): web service endpoint url\n",
    "        service_key(str): Service key. None for local deployment.\n",
    "        parameters (dict): Additional request paramters in dictionary. Default is {}.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        str: serialized result \n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    if service_key is None:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "    else:\n",
    "        headers = {'Content-Type': 'application/json',\n",
    "                   \"Authorization\": ('Bearer ' + service_key)}\n",
    "    payload = []\n",
    "    encoded = None\n",
    "    \n",
    "    # Read image\n",
    "    with open(image,'rb') as f:\n",
    "        image_buffer = BytesIO(f.read()) ## Getting an image file represented as a BytesIO object\n",
    "        \n",
    "    # Convert your image to base64 string\n",
    "    # image_in_base64 : \"b'{base64}'\"\n",
    "    encoded = base64.b64encode(image_buffer.getvalue())\n",
    "    image_request = {\"image_in_base64\": \"{0}\".format(encoded), \"parameters\": parameters}\n",
    "    payload.append(image_request)\n",
    "    body = json.dumps(payload)\n",
    "    r = requests.post(service_endpoint_url, data=body, headers=headers)\n",
    "    try:\n",
    "        result = json.loads(r.text)\n",
    "        json.loads(result[0])\n",
    "    except:\n",
    "        raise ValueError(\"Incorrect output format. Result cant not be parsed: \" + r.text)\n",
    "    return result[0]\n",
    "\n",
    "# Test with images\n",
    "image = test_set.images[0].storage_path # A local image file\n",
    "score_image_with_http(image, service_endpoint_url, service_key) # Local scoring the service_key is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse serialized result from webservice\n",
    "The result from the webserice is in json string. You can parse it the with different DNN model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_or_url = test_set.images[0].storage_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse result from json string\n",
    "import numpy as np\n",
    "parsed_result = CNTKTLModel.parse_serialized_result(serialized_result_in_json)\n",
    "print(\"Parsed result:\", parsed_result)\n",
    "# Map result to image class\n",
    "class_index = np.argmax(np.array(parsed_result))\n",
    "print(\"Class index:\", class_index)\n",
    "dnn_model.class_map\n",
    "print(\"Class label:\", dnn_model.class_map[class_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# APPENDIX \n",
    "\n",
    "# (A) Bing image download\n",
    "\n",
    "The dataset object provides functionality to download images using the [Bing Image Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/). Two types of search queries are supported: (i) regular text queries; and (ii) image URL queries. These queries need to be provided inside a json-encoded text file. In addition to the search queries, also their class label has to be specified (see this [sample json file](./misc/sample.json)). Furthermore, a Context object needs to be created explicitly and contain the Bing Image Search API key which requires a Bing Image Search API subscription. \n",
    "\n",
    "\n",
    "\n",
    "# (B) How to improve accuracy\n",
    "The Computer Vision toolkit (CVTK) is shown to give good results for a wide variety of datasets. However, as is true for most machine learning projects, getting the best possible results for a new dataset requires careful parameter tuning, as well as evaluating different design decisions. The following sections provide guidance on how to improve accuracy on a given dataset, ie. what parameters are most promising to optimize first, what values for these parameters one should try, and what pitfalls to avoid.\n",
    "\n",
    "Generally speaking, training Deep Learning models comes with a trade-off between training time versus model accuracy. The CVTK has pre-set default parameters (see 1st row in the table below) which focus on fast training speed while typically producing high accuracy models. This accuracy can often be improved further using e.g. higher image resolution or deeper models, however at the cost of increasing training time by a factor of 10x or more.\n",
    "\n",
    "Our recommendation is to first work with the default parameters, train a model, inspect the results, correct ground truth annotations as needed, and only then try parameters which slow down training time (see table below suggested parameter values). An understanding of these parameters while technically not necessary is however recommended.\n",
    "\n",
    "\n",
    "### Best-practice / Pitfalls / Tips:\n",
    "* Data quality: the training and test sets should be of high quality. That is, the images are annotated correctly, ambiguous images removed (for example where it is unclear to a human eye if the image shows a tennis ball or a lemon), and the attributes are mutually exclusive (that is, each image belongs to exactly one attribute).\n",
    "* Before refining the DNN, a SVM classifier should be trained using a pre-trained and fixed DNN as featurizer. This is supported in CVTK and does not require long to train since the DNN itself is not modified. Even this simple approach often achieves good accuracies and hence represents a strong baseline. The next step is then to refine the DNN which should give better accuracy.\n",
    "* If the object-of-interest is small in the image then Image classification approaches are known to not work well. In such cases, consider using an object detection approach such as CVTK's Faster R-CNN based on Tensorflow.\n",
    "* The more training data the better. As a rule-of-thumb, one should have at least 100 examples for each class, ie. 100 images for \"dog\", 100 images for \"cat\", etc. Training a model with less images is possible but might not produce good results.\n",
    "* The training images need to reside locally on the machine with the GPU, and be on an SSD drive (not a HDD). Otherwise, latency from image reading can drastically reduce training speed (by even a factor of 100x).\n",
    "\n",
    "\n",
    "### Parameters to optimize:\n",
    "Finding optimal values for these parameters is important and can often improve accuracy significantly:\n",
    "* Learning rate (`lr_per_mb`): The arguably most important parameter to get right. If the accuracy on the training set after DNN refinement is above ~5% then most likely the learning rate is either too high, or the number of training epochs too low. Especially with small datasets, the DNN tends to over-fit on the training data, however in practice this still leads to good models on the test set. We typically use 15 epochs where the inital learning rate is reduced twice; training using more epochs can in some cases improve performance.\n",
    "* Input resolution (`image_dims`): The default image resolution is 224x224 pixels. Using higher image resolution of, for example, 500x500 pixels or 1000x1000 pixels can significantly improve accuracy but slows down DNN refinement. Note that CVTK expects the input resolution to be a tuple of (color-channels, image-width, image-height), e.g. (3, 224, 224), where the number of color channels has to be set to 3 (the Red-Green-Blue bands).\n",
    "* Model architecture(`base_model_name`): Try using deeper DNNs such as ResNet-34 or ResNet-50 instead of the default ResNet-18 model. The Resnet-50 model is not only deeper, but its output of the penultimate layer is of size 2048 floats (vs. 512 floats of the ResNet-18 and ResNet-34 models). This increased dimensionality can be especially beneficial when keeping the DNN fixed and instead training an SVM classifier.\n",
    "* Minibatch size (`mb_size`): High minibatch sizes will lead to faster training time however at the expense of an increased DNN memory consumption. Hence, when selecting deeper models (E.g. ResNet-50 versus ResNet-18) and/or higher image resolution (500\\*500 pixels versus 224\\*224 pixels), one typically has to reduce the minibatch size to avoid out-of-memory errors. When changing the minibatch size, often also the learning rate needs to be adjusted as can be seen in the table below.\n",
    "* Drop-out rate (`dropout_rate`) and L2-regularizer (`l2_reg_weight`): DNN over-fitting can be reduced by using a dropout rate of 0.5 (default is 0.5 in CVTK) or more, and by increasing the regularizer weight (default is 0.0005 in CVTK). Note though that especially with small datasets DNN over-fitting is hard and often impossible to avoid.\n",
    "\n",
    "\n",
    "### Parameter explanation:\n",
    "- Learning rate: step size used during gradient descent learning. If set too low then the model will take many epochs to train, if set too high then the model will not converge to a good solution. Note that typically a schedule is used where the learning rate is reduced after a certain number of epochs. E.g. the learning rate schedule `[0.05]*7 + [0.005]*7 + [0.0005]` corresponds to using an initial learning rate of 0.05 for the first 7 epochs, followed by a 10x reduced learning rate of 0.005 for another 7 epochs, and finally fine-tuning the model for a single epoch with a 100x reduced learning rate of 0.0005.\n",
    "- Minibatch size: GPU's can process multiple images in parallel to speed up computation. These parallel processed images are also referred as a minibatch. The higher the minibatch size the faster training will be, however at the expense of a significantly increased DNN memory consumption.\n",
    "\n",
    "### Suggested parameters values:\n",
    "The table below provides different parameter sets which were shown to produce high accuracy models on a wide variety of image classification tasks. The optimal parameters depend on the specific dataset and on the exact GPU used, hence the table should be seen as a guideline only. After trying these parameters, consider also image resolutions of more than 500x500 pixels, or deeper models such as Resnet-101 or Resnet-152.\n",
    "\n",
    "The first row in the table corresponds to the default parameters which are set inside CVTK. All other rows take longer to train (indicated in the first column) however at the benefit of increased accuracy (see the second column for the average accuracy over three internal datasets). For example, the parameters in the last row take 5-15x longer to train, however resulted in significantly increased (averaged) accuracy on three internal test sets from 82.6% to 92.8%.\n",
    "\n",
    "Deeper models and higher input resolution take up more DNN memory, and hence the minibatch size needs to be reduced with increased model complexity to avoid out-of-memory-errors. As can be seen in the table below, we found it beneficial to decrease the learning rate by a factor of two whenever decreasing the minibatch size by the same multiplier. Note that the minibatch size might need to get reduced further on GPUs with smaller amounts of memory.\n",
    "\n",
    "| Training time (rough estimate) | Example accuracy | Minibatch size (`mb_size`) | Learning rate (`lr_per_mb`) | Image resolution (`image_dims`) | DNN architecture (`base_model_name`) |\n",
    "|------------- |:-------------:|:-------------:|:-----:|:-----:|:---:|\n",
    "| 1x (reference) | 82.6% | 32 | [0.05]\\*7  + [0.005]\\*7  + [0.0005]  | (3, 224, 224) | ResNet18_ImageNet_CNTK |\n",
    "| 2-5x    | 90.2% | 16 | [0.025]\\*7 + [0.0025]\\*7 + [0.00025] | (3, 500, 500) | ResNet18_ImageNet_CNTK |\n",
    "| 2-5x    | 87.5% | 16 | [0.025]\\*7 + [0.0025]\\*7 + [0.00025] | (3, 224, 224) | ResNet50_ImageNet_CNTK |\n",
    "| 5-15x        | 92.8% |  8 | [0.01]\\*7  + [0.001]\\*7  + [0.0001]  | (3, 500, 500) | ResNet50_ImageNet_CNTK |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© 2018 Microsoft. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
